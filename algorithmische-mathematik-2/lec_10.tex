%! TEX root = ./master.tex
\lecture[Unabhängige Ereignisse und Zufallsvariablen. Kovarianz. (Un-) Korreliertheit. Additivität der Varianz für unkorrelierte Zufallsvariablen. ]{Mi 12 Mai 2021 10:16}{Unabhängigkeit von Ereignissen und Zufallsvariablen.}
\begin{example}
    Sei $\mathcal{S} = \left \{0,1\right\}$ und $α,β\in (0,1)$. Setze
    \[
        P = \begin{pmatrix} 1 - α & α \\ β & 1-β \end{pmatrix} 
    .\] 
    Oft machen wir eine graphische Darstellung:
    Wir wollen also $P^n$ ausrechnen, um das Verhalten der Markovkette zu studieren. 
     \begin{enumerate}[label=\protect\circled{\alph*}]
        \item Man könnte nun $P$ diagonalisieren, also  $P = U \Lambda U^{-1}$, wobei $A$ Diagonalform hat, so ist $P^n = U \Lambda ^n U^{-1}$, und die Potenzen von $\Lambda$ können wir leicht berechnen.
        \item  $P^n$ ist stochastisch, also ist auch  $P^n(0,0) + P^n(0,1)=1$,  $P^n(1,1) + P^n(1,0) = 1$, weil  $P^n$ stochastisch ist. Wir wollen uns nun, welchen Wert $P^n(0,0)$ hat, dazu machen wir den Rekursion Ansatz:
            \begin{itemize}
                \item Sind wir nach $n-1$ Schritten wieder bei der  $0$, so müssen wir von  $0$ nach  $0$ gehen
                \item Sind wir nach  $n-1$ Schritten bei der  $1$, so müssen wir im  $n$-ten von  $1$ nach  $0$ gehen
            \end{itemize}
            Also ergibt sich
            \begin{IEEEeqnarray*}{rCl}
                P^n(0,0) &=& P^{n-1}(0,0)\cdot P(0,0) + P^{n-1}(0,1) \cdot P(1,0)  \\
                         & = & P^{n-1}(0,0)(1-α)+ \underbrace{P^{n-1}(0,1)}_{= 1-P^{n-1}(0,0)}\cdot β \\
                         & = & P^{n-1}(0,0)(1-α-β) + β
            \end{IEEEeqnarray*}
            Als Rekursion suhcne wir also eine Funktion $n \mapsto f(n)$, sodass
             \[
            \begin{cases}
                f(n) & = β + (1-α-β) f(n-1) \\
                f(1) & = 1-α
            \end{cases}
            .\] 
            Als Lösung ergibt sich (Theorie der linearen Rekursionen!):
            \[
                f(n) = \frac{β}{α+β} + \frac{α}{α+β}(1-α-β)^n
            .\] 
            Damit ergibt sich
            \[
                P^n = \begin{pmatrix} \frac{β}{α+β} & \frac{α}{α+β} \\ \frac{β}{α+β} & \frac{α}{α+β} \end{pmatrix}  + (1-α-β)^n \begin{pmatrix} \frac{α}{α+β} & \frac{-α }{\alpha + \beta} \\ \frac{-\beta }{\alpha + \beta} & \frac{\beta}{\alpha + \beta} \end{pmatrix} 
            .\] 
            Wir beschränken uns auf $α,β \in (0,1)$. Da $(1-α-β)^n \to 0$ exponentiell schnell, ergibt sich
            \[
            \lim_{n \to \infty} P^n = \begin{pmatrix} \frac{β}{α+β} & \frac{α}{α+β} \\ \frac{β}{α+β} & \frac{α}{α+β} \end{pmatrix} 
            .\] 
            Wir stellen fest, dass diese Matrix Rang 1 hat. Ebenso ergibt sich
\[
\lim_{n \to \infty} \mu_0 P^n = \begin{pmatrix} \frac{β}{α+β}, \frac{α}{α+β} \end{pmatrix} 
.\] 
und dieser ist von $μ_0$ unabhängig.
    \end{enumerate}
\end{example}
\todo{Grafik}
\subsection{Unabhängige Zufallsvariablen}
\subsubsection{Unabhängige Ereignisse}

\begin{definition}[Unabhängigkeit]\label{def:unabhängigkeit-von-ereignissen}
    Eine Familie von Ereignissen $\left \{A_k\right\} _{k \in I}$ heißt unabhängig, falls 
    \[
        P(A_{i_1}\cap \ldots\cap A_{i_n}) = \prod_{k=1}^n P(A_{i_k})
    .\] 
    für alle verschiedenen $i_{1},\ldots.i_{n}\in I$ und für alle $n\leq \abs{I}$.
\end{definition}
Seien $A,B\in \mathcal{F}$, sodass $\mathbb{P}(A) \neq  0, \mathbb{P}(B) \neq 0$. Dann sind $A$ und  $B$ unabhängig gwd
 \[
     \mathbb{P}(A\mid B) = \mathbb{P}(A) \iff  \mathbb{P}(B\mid A) = \mathbb{P}(B)
.\]
\begin{warning}
    Falls $\mathbb{P}(A_i \cap  A_j) = \mathbb{P}(A_i) \cdot \mathbb{P}(A_j)$ für $i\neq j \in I$, so folgt noch nicht, dass $\left \{A_i\right\} _{i \in I}$ unabhängig ist!
\end{warning}
\begin{example}
    Betrachte 3 faire Münzen, also
    \[
        \Omega = \left \{0,1\right\} ^3 f \left \{(\omega_1,\omega_2,\omega_3) \mid  \omega_i \in  \left \{0,1\right\} \text{ für } i=1,2,3,\right\} 
    .\] 
    wobei 
    \[
    \omega_i = \begin{cases}
        0 & \text{Münze $i$ ist Kopf} \\
        1 & \text{Münze $i$ ist Zahl}
    \end{cases}
    .\] 
    Wir setzen $\mathcal{F} = \mathcal{P}(\Omega)$ und $\mathbb{P}$ als die Gleichverteilung und betrachten die Ereignisse
    \[
    A_1 = \left \{\omega_1 = \omega_2\right\} \qquad A_2 = \left \{\omega_2 = \omega_3\right\}  \qquad A_3 = \left \{\omega_3 = \omega_1\right\} 
    .\] 
    und erhalten unmittelbar
     \[
         \mathbb{P}(A_i) = \frac{4}{8}=\frac{1}{2} \qquad \mathbb{P}(A_i \cap A_j) = \frac{2}{8} = \frac{1}{4} = \mathbb{P}(A_i)\cdot \mathbb{P}(A_j) \; \forall i\neq j
    .\] 
    allerdings ist auch
    \[
        \mathbb{P}(A_1\cap A_2\cap A_3) = \frac{2}{8} \neq  \mathbb{P}(A_1) \mathbb{P}(A_2) \mathbb{P}(A_3)
    .\] 
    also sind die Ereignisse nicht unabhängig, allerdings paarweise unabhängig. Dies liegt daran, dass $A_1 \land A_2 \implies A_3$ etc.
\end{example}
\subsubsection{Unabhängige Zufallsvariablen}
\todo{Skizze}

\begin{definition}[Gemeinsame Verteilung]\label{def:gemeinsame-verteilung}
    Seien $X_k : \Omega\to \mathcal{S}_k$ für $1\neq k\leq n$ diskeret Zufallsvariablen auf $(\Omega,\mathcal{F},\mathbb{P})$. Die Verteilung $\mu_{X_1,\ldots,X_n}$ der Zufallsvektoren $(X_1,\ldots,X_n)$ mit Massenfunktion
    \[
        p_{X_1,\ldots,X_n}(a_1,\ldots,a_n) := \mathbb{P}(X_1 = a_1, \ldots, X_n = a_n)
    .\] 
    heißt die \vocab{gemeinsame Verteilung} von $X_1,\ldots,X_n$. 
\end{definition}

\begin{definition}[Unabhängigkeit]\label{def:unabhängigkeit}
    Die diskreten Zufallsvariablen $X_1,\ldots,X_n$ heißen \vocab{unabhängig}, falls
    \[
        \mathbb{P}(X_1=a_1,\ldots,X_n = a_n) = \prod_{k=1}^n \mathbb{P}(X_k = a_k)
    .\]
    für alle $a_1\in \mathcal{S}_1\ldots,a_n \in \mathcal{S}_n$.
\end{definition}

\begin{theorem}
    Die folgenden Aussagen sind äquivalent
    \begin{enumerate}[label=\protect\circled{\alph*}]
        \item Die Zufallsvariablen $X_1,\ldots,X_n$ sind unabhängig
        \item $p_{X_1,\ldots,X_n}(a_1,\ldots,a_n) = \prod_{k=1}^n p_{X_k}(a_k)$ für alle $a_k \in S_k, 1\leq k\leq n$.
        \item \emphasize{$\mu_{X_1,\ldots,X_n} = \mu_{X_1} \otimes  \ldots \otimes \mu_{X_n}$} (Produktmass)
        \item Die Ereignisse $\left \{X_1\in A_1\right\},\ldots,\left \{X_k \in A_k\right\}  $ sind unabhängig für alle $A_1\subset \mathcal{S}_1,\ldots,A_n \subset \mathcal{S}_n$
        \item Die Ereignisse $\left \{X_1 = a_1\right\} ,\ldots,\left \{X_n = a_n\right\} $ sind unabhängig für $a_1\in \mathcal{S}_1,\ldots,a_n\in \mathcal{S}_n$
    \end{enumerate}
\end{theorem}

\begin{proof}
    \begin{itemize}
        \item $\circled{a} \iff \circled{b}$ folgt sofort aus \autoref{def:gemeinsame-verteilung}.
            \item $\circled(b) \iff \circled{c}$ folgt aus \autoref{def:produktmodell}, da $\mathbb{P} = \mathbb{P}_1 \otimes  \mathbb{P}_2$ genau bedeutet, dass für $A_1\subset \Omega_1,A_2\subset \Omega_2$ 
                    \[
                        \mathbb{P}(A_1\times A_2) = \mathbb{P}_1(A_1) \mathbb{P}_2(A_2)
                    .\] 
gilt
\item $\circled{c} \implies \circled{d}$ Seien $1\leq i_1 < i_2 < \ldots < i_{k} \leq n$ und $A_{i_{k}\subset \mathcal{S}_{i_k}}$ für $1\leq k\leq n$. Für $i\not\in \left \{i_1,\ldots,i_k\right\} $ setzen wir $A_i =: \mathcal{S}_i$, dann erhalten wir
    \begin{IEEEeqnarray*}{rCl}
        \mathbb{P}(X_{i_1} \in A_{i_1}, \ldots, X_{i_k} \in  A_{i_k}) & = & \mathbb{P}(X_1\in A_1,\ldots,X_{n}\in A_{n}) \\
                                                                      & = & \mu_{X_{i_1},\ldots,X_{i_k}} (A_1\times \ldots\times A_n) \\
                                                                      & \stackrel{\autoref{thm:stufenmodell}}{=} &  \prod_{k=1}^n \mu_{X_k} (A_k) = \prod_{k=1}^m \mathbb{P}(X_{i_k} \in A_{i_k})\\
    \end{IEEEeqnarray*}

\item $\circled{d} \implies \circled{e}$ Wähle $A_i = \left \{a_/\right\} $, dann sind wir sofort fertig.
\item $\circled{e} \implies \circled{a}$. Das folgt sofort aus \autoref{def:unabhängigkeit-von-ereignissen}.
    \end{itemize}
\end{proof}
\todo{Gleichung nochmal durchgehen}

\subsubsection{Die Kovarianz}
Seien $X_1,\ldots,X_n$ Zufallsvariablen auf $(\Omega,\mathcal{F},\mathbb{P})$. Wir wissen bereits, dass
\[
    \mathbb{E}(X_1 + \ldots + X_n) = \sum_{k=1}^n \mathbb{E}(X_k)
.\] 
nach Linearität des Erwartungswerts.
\begin{question}
Was ist allerdings $\Var(X_1 + \ldots + X_n)$?
\end{question}
\begin{definition}[Kovarinz]\label{def:kovarianz}
    Seien $X,Y$ zwei Zufallsvariablen auf  $(\Omega,\mathcal{F},\mathbb{P})$. Dann ist
    \[
        \Cov(X,Y) := \mathbb{E}(X\cdot Y) - \mathbb{E}(X) \cdot \mathbb{E}(Y)
    .\] 
    die \vocab{Kovarianz} von $X$ und  $Y$.
\end{definition}

\begin{remark}
    Für $X = Y$ ist  $\Cov(X,X) = \Var(X)$.
\end{remark}

\begin{lemma}\label{lm:kovarianz-ist-null-für-unabhängige-variablen}
    Seien $X_1,X_2$ zwei unabhängige Zufallsvariablen. Dann ist
    \[
        \Cov(X_1,X_2) = 0
    .\] 
\end{lemma}

\begin{warning}
    Die Umkehrung des Lemmas gilt nicht!
\end{warning}

\begin{proof}[Beweis von \autoref{lm:kovarianz-ist-null-für-unabhängige-variablen}]
    Wir erhalten
    \begin{IEEEeqnarray*}{rCl}
        \mathbb{E}(X_1\cdot X_2) &=& \sum_{\substack{a_1\in \mathcal{S}_1 \\a_2\in \mathcal{S}_2}} \mathbb{P}(X_1 = a_1, X_2 = a_2) a_1a_2  \\
         & = & \sum_{\substack{ a_1\in \mathcal{S}_1 \\ a_2 \in \mathcal{S}_2} } \mathbb{P}(X_1 = a_1) \mathbb{P}(X_2 = a_2) a_1 a_2 \\
         & = & \sum_{a_1\in \mathcal{S}_{1}} a\mathbb{P}(X_1 = a_1) \underbrace{\sum_{a_2 \in S_2} a_2\mathbb{P}(X_2 = a_2)}_{= \mathbb{E} (X_2)} \\
         & = & \mathbb{E}(X_1) \sum_{a_1\in \mathcal{S}_1} a_1\mathbb{P}(X_1 = a_1) \\
         & = & \mathbb{E}(X_2) \mathbb{E}(X_1)
    \end{IEEEeqnarray*}
\end{proof}

\begin{definition}[Unkorreliert]\label{def:unkorrelation}
    Zwei Zufallsvariablen $X_1,X_2$ sind \vocab{unkorreliert}, falls $\Cov(X_1,X_2) = 0$. 
\end{definition}

\begin{warning}
    Wir haben also Unabhängigkeit $\implies$ Unkorreliertheit, aber nicht die Umkehrung.
\end{warning}

\begin{lemma}\label{lm:varianz-von-summe}
    Seien $X_1,\ldots,X_n$ Zufallsvariablen auf $(\Omega,\mathcal{F},\mathbb{P})$. Dann ist
    \[
        \Var(X_1+ \ldots + X_n) = \sum_{k=1}^n \Var(X_k) + 2 \sum_{1\leq i<j\leq n} \Cov(X_i,X_j)
    .\] 
\end{lemma}

\begin{dcorollary}
    Für unabhängige (oder auch unkorrelierte) Zufallsvariablen $X_1,\ldots,X_n$ ist
    \[
        \Var(X_1+ \ldots + X_n) = \sum_{k=1}^n \Var(X_k)
    .\] 
\end{dcorollary}
\begin{proof*}
    Einsetzen in \autoref{lm:varianz-von-summe} und verwenden von $\Cov(X_i, X_j) =0$ für $i\neq j$ liefert sofort das Ergebnis.
\end{proof*}

\begin{example}
    Seien $X_1,\ldots,X_n \sim \Ber(p)$ unabhängige Zufallsvariablen. Wir wissen bereits, dass $X = X_1 + \ldots + X_n \sim \Bin(n,p)$ binomialverteilt ist. Also ergibt sich auch für die Varianz:
    \[
        \Var(X) = n\cdot \Var(X_1) = n p(1-p)
    .\] 
    - ohne eine lange Rechnung machen zu müssen, wie wir es bisher taten.
\end{example}

\begin{proof}[Beweis von \autoref{lm:varianz-von-summe}]
    \begin{IEEEeqnarray*}{rCl}
        \Var(X_1+\ldots+X_n)&  = &\mathbb{E}\left( \left( \sum_{i=1}^n X_i \right) ^2 \right) - \left( \sum_{i=1^n} \mathbb{E}(X_i) \right) ^2 \\
                            & = & \mathbb{E}\left( \sum_{k,l=1}^n X_k\cdot X_l \right) - \sum_{k,l=1}^n \mathbb{E}(X_k)\mathbb{E}(X_l) \\
                            & = & \sum_{k=1}^n \left( \mathbb{E}(X_k^2) - \left( \mathbb{E}(X_k) \right) ^2 \right)  + 2 \sum_{1\leq k<l\leq n}^n \left(\mathbb{E}(X_k\cdot X_l) - \mathbb{E}(X_k)\cdot \mathbb{E}(X_l)\right) \\
                            & = & \sum_{k=1}^n \Var(X_k) + 2 \sum_{k=1}^n \Cov(X_k,X_l)
    \end{IEEEeqnarray*}
\end{proof}

\begin{question}
    Wenn $X_1,\ldots,X_n$ Zufallsvariablen sind, kennen wir bereits ihren Erwartungswert als ungefähre Näherung für ihre Summe. Genauer wollen wir aber wissen, was
    \[
        \mathbb{P}(\abs{(X_1 + \ldots + X_n) -  (\mathbb{E} (X_1) + \ldots + \mathbb{E} (X_n))} \geq ε)
    .\] 
    ist, für gegebene $ε$ (d.h. wie hoch ist die Wahrscheinlichkeit, dass das tatsächliche Ergebnis unseren Zufallsexperminets um mindestens $ε$ vom Erwartungswert abweicht?). Insbesondere interessieren wir uns darüber, wann dieser Wert klein ist (dann ist der Erwartungswert 'zuverlässiger')
\end{question}
